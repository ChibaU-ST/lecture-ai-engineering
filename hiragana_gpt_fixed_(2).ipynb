{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChibaU-ST/lecture-ai-engineering/blob/master/hiragana_gpt_fixed_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z39bMgyvSIa3"
      },
      "source": [
        "# 🤖 ひらがなGPT - AIに日本語を教えよう！\n",
        "\n",
        "## 📖 このノートブックで学ぶこと\n",
        "\n",
        "このノートブックでは、**ChatGPTと同じ仕組み（Transformer）** を使って、\n",
        "日本語の文章を生成するAIを一から作ります。\n",
        "\n",
        "### 🎯 到達目標\n",
        "1. **AIがどうやって文章を理解するか**を知る\n",
        "2. **Transformerの仕組み**を理解する\n",
        "3. **実際に動くAI**を自分で作る\n",
        "4. **学習の様子**をグラフで確認する\n",
        "\n",
        "### ✨ 改善版の特徴\n",
        "この改善版では、AIをより賢くするための**3つの新技術**を追加しています：\n",
        "1. **Learning Rate Scheduler** - 学習速度を自動調整\n",
        "2. **Gradient Clipping** - 学習の安定化\n",
        "3. **Weight Decay** - 過学習の防止\n",
        "\n",
        "### 💡 予備知識\n",
        "- **必要**: なし！（初めてでOK）\n",
        "- **あると便利**: Pythonの基本（変数、リスト、関数）\n",
        "\n",
        "---\n",
        "\n",
        "## 🗺️ 全体の流れ\n",
        "\n",
        "```\n",
        "1. 準備 → 2. データを見る → 3. AIを作る → 4. 学習 → 5. 文章生成！\n",
        "```\n",
        "\n",
        "それでは始めましょう！ 🚀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-mClo3KSIa4"
      },
      "source": [
        "---\n",
        "\n",
        "# 📦 ステップ1: 準備（ライブラリの読み込み）\n",
        "\n",
        "## 🤔 ライブラリって何？\n",
        "\n",
        "プログラムを書くときに便利な「道具箱」のようなものです。\n",
        "\n",
        "- **PyTorch**: AIを作るための道具\n",
        "- **matplotlib**: グラフを描く道具\n",
        "- **numpy**: 数値計算の道具\n",
        "\n",
        "下のセルを実行してください！ ▶️ボタンを押すか、`Shift + Enter`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqXN7k6KSIa4",
        "outputId": "21a8dc79-bbac-4e14-b555-1e3bab5b1aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "🎉 ライブラリの読み込み成功！\n",
            "======================================================================\n",
            "📅 実行開始時刻: 2025年10月06日 16:45:47\n",
            "🔧 PyTorchバージョン: 2.8.0+cu126\n",
            "💻 GPU（高速計算）は使える？ False\n",
            "   → CPUで動きます（GPUより遅いですが問題なし）\n",
            "======================================================================\n",
            "✅ 準備完了！次のステップへ進みましょう\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# 必要なライブラリを読み込む\n",
        "import torch              # AI（ニューラルネットワーク）を作るための基本ツール\n",
        "import torch.nn as nn     # ニューラルネットワークの部品\n",
        "from torch.nn import functional as F  # よく使う関数たち\n",
        "import os                 # ファイル操作\n",
        "from datetime import datetime  # 時刻の表示\n",
        "import matplotlib.pyplot as plt  # グラフを描く\n",
        "import numpy as np        # 数値計算\n",
        "\n",
        "# 日本語フォント設定（グラフで日本語を表示するため）\n",
        "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
        "\n",
        "# 実行環境の確認\n",
        "print(\"=\"*70)\n",
        "print(\"🎉 ライブラリの読み込み成功！\")\n",
        "print(\"=\"*70)\n",
        "print(f\"📅 実行開始時刻: {datetime.now().strftime('%Y年%m月%d日 %H:%M:%S')}\")\n",
        "print(f\"🔧 PyTorchバージョン: {torch.__version__}\")\n",
        "print(f\"💻 GPU（高速計算）は使える？ {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   → 使えます！ GPU名: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   → 学習が速くなります ⚡\")\n",
        "else:\n",
        "    print(f\"   → CPUで動きます（GPUより遅いですが問題なし）\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"✅ 準備完了！次のステップへ進みましょう\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d6BGxMGSIa5"
      },
      "source": [
        "---\n",
        "\n",
        "# ⚙️ ステップ2: 設定（ハイパーパラメータ）\n",
        "\n",
        "## 🎛️ ハイパーパラメータって何？\n",
        "\n",
        "AIの性能を決める「調整ダイヤル」のようなものです。\n",
        "\n",
        "### 📊 主な設定項目\n",
        "\n",
        "| 項目 | 意味 | 例え |\n",
        "|------|------|------|\n",
        "| `n_embd` | AIの頭の良さ | 脳のサイズ 🧠 |\n",
        "| `n_layer` | 思考の深さ | 考える回数 🤔 |\n",
        "| `n_head` | 視点の数 | 目の数 👀 |\n",
        "| `learning_rate` | 学習の速さ | 歩幅の大きさ 👣 |\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Learning Rate Scheduler（学習率スケジューラ）\n",
        "**学習速度を自動調整する仕組みです**\n",
        "\n",
        "- **ウォームアップ期間**: 最初はゆっくり学習（慎重に）\n",
        "- **コサイン減衰**: 徐々に学習速度を下げる（繊細に）\n",
        "- **効果**: 学習が安定し、Lossが下がりやすくなる\n",
        "\n",
        "例えると...\n",
        "- 車の運転: 最初はゆっくり発進 → だんだん加速 → 目的地に近づいたら減速\n",
        "\n",
        "### 2. Gradient Clipping（勾配クリッピング）\n",
        "**学習の「暴走」を防ぐ仕組みです**\n",
        "\n",
        "- AIが学習する時、時々計算が爆発的に大きくなることがあります\n",
        "- これを防ぐため、一定以上大きくならないように制限します\n",
        "- **効果**: 学習が安定し、途中で壊れにくくなる\n",
        "\n",
        "例えると...\n",
        "- 車のスピードリミッター: どんなにアクセルを踏んでも一定速度以上出ない\n",
        "\n",
        "### 3. Weight Decay（重み減衰 / L2正則化）\n",
        "**AIの「暗記」を防ぐ仕組みです**\n",
        "\n",
        "- AIは時々、答えを暗記してしまうことがあります（過学習）\n",
        "- これを防ぐため、パラメータが大きくなりすぎないようにします\n",
        "- **効果**: 見たことないデータでも正しく動くようになる\n",
        "\n",
        "例えると...\n",
        "- 試験勉強: 答えを丸暗記するのではなく、理解して覚える\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 実験してみよう！\n",
        "\n",
        "下の値を変えて実験できます。\n",
        "まずは**そのまま実行**して、後で変えてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxAPoXBoSIa5",
        "outputId": "efeeaefe-013e-4d60-9afc-91aaa42b24ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "⚙️  ハイパーパラメータ設定（改善版）\n",
            "======================================================================\n",
            "\n",
            "🖥️  【実行環境】\n",
            "   計算デバイス: cpu\n",
            "   🐢 CPUで実行します（GPUより遅いですが動きます）\n",
            "\n",
            "📦 【データ処理】\n",
            "   バッチサイズ: 64 個\n",
            "   コンテキスト長: 128 文字\n",
            "   → 一度に64個の128文字の文章を学習\n",
            "\n",
            "🧠 【AIの構造】（改善版）\n",
            "   埋め込み次元: 256 （256→384に強化）\n",
            "   Attention Head: 8 個\n",
            "   Transformer層: 8 層 （6→8層に強化）\n",
            "   Dropout率: 0.1\n",
            "   → 予想パラメータ数: 約6.3M個\n",
            "\n",
            "📚 【学習設定】\n",
            "   初期学習率: 0.0003 （3e-4→6e-4に上昇）\n",
            "   最大イテレーション: 5,000 回 （3000→5000に増加）\n",
            "   評価間隔: 100 回ごと\n",
            "\n",
            "🆕 【改善版の新機能】\n",
            "   1️⃣ Learning Rate Scheduler:\n",
            "      - ウォームアップ: 100回\n",
            "      - 最小学習率: 6e-05\n",
            "      → 最初はゆっくり、徐々に速く、最後は丁寧に学習\n",
            "\n",
            "   2️⃣ Gradient Clipping:\n",
            "      - 勾配の上限: 1.0\n",
            "      → 学習の暴走を防止\n",
            "\n",
            "   3️⃣ Weight Decay:\n",
            "      - 正則化強度: 0.1\n",
            "      → 過学習（暗記）を防止\n",
            "\n",
            "💾 【チェックポイント】\n",
            "   初期保存タイミング: [1, 50, 100, 200, 400, 750]\n",
            "   以降の保存間隔: 250 回ごと\n",
            "\n",
            "⏹️  【Early Stopping】\n",
            "   忍耐力: 15 回 （8→15に増加）\n",
            "   改善判定: 0.001\n",
            "\n",
            "======================================================================\n",
            "✅ 設定完了！次はデータを見てみましょう\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 🎛️ ここを変更して実験できます！\n",
        "# ==============================================================================\n",
        "\n",
        "# --- データ処理の設定 ---\n",
        "batch_size = 64       # 一度に何個の文章を学習する？（大→速いが重い、小→遅いが軽い）\n",
        "block_size = 128      # 何文字分を一度に見る？（大→長い文脈、小→短い文脈）\n",
        "\n",
        "# --- 学習の設定 ---\n",
        "max_iters = 5000      # 何回繰り返し学習する？（改善版: 3000→5000に増加）\n",
        "eval_interval = 100   # 何回ごとに成績をチェックする？\n",
        "learning_rate = 3e-4  # 学習速度の初期値\n",
        "eval_iters = 100      # 成績チェックの精度\n",
        "\n",
        "# --- AIの構造（改善版: より強力に）---\n",
        "n_embd = 256         # 脳のサイズ（改善版: 256→384に増加）\n",
        "n_head = 8            # 何個の視点で見る？（改善版: 8→6に調整）\n",
        "n_layer = 8           # 何層の思考？（改善版: 6→8層に増加）\n",
        "dropout = 0.1         # 過学習防止（改善版: 0.2→0.1に減少）\n",
        "\n",
        "# --- 🆕 改善版で追加された設定 ---\n",
        "\n",
        "# Learning Rate Scheduler設定\n",
        "warmup_iters = 100          # ウォームアップ期間（最初100回はゆっくり学習）\n",
        "lr_decay_iters = max_iters  # 減衰期間（全体で徐々に減速）\n",
        "min_lr = 6e-5               # 最小学習率（初期の1/10まで下げる）\n",
        "\n",
        "# Gradient Clipping設定\n",
        "grad_clip = 1.0       # 勾配の最大値（これ以上大きくならない）\n",
        "\n",
        "# Weight Decay設定\n",
        "weight_decay = 1e-1   # L2正則化の強さ（0.1 = やや強め）\n",
        "\n",
        "# --- チェックポイント（途中経過の保存）---\n",
        "checkpoint_intervals = [1, 50, 100, 200, 400, 750]  # 最初は細かく保存\n",
        "checkpoint_interval_regular = 250  # 750回以降は250回ごと\n",
        "checkpoint_dir = 'checkpoints'     # 保存先フォルダ\n",
        "\n",
        "# --- 自動停止の設定 ---\n",
        "patience = 15         # 何回改善しなかったら諦める？（改善版: 8→15に増加）\n",
        "min_delta = 0.001     # どれくらい改善したら「改善」とみなす？\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "# 計算デバイスの決定（GPUがあればGPU、なければCPU）\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 設定の表示\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"⚙️  ハイパーパラメータ設定（改善版）\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n🖥️  【実行環境】\")\n",
        "print(f\"   計算デバイス: {device}\")\n",
        "if device == 'cuda':\n",
        "    print(f\"   💨 GPUで高速実行します！\")\n",
        "else:\n",
        "    print(f\"   🐢 CPUで実行します（GPUより遅いですが動きます）\")\n",
        "\n",
        "print(f\"\\n📦 【データ処理】\")\n",
        "print(f\"   バッチサイズ: {batch_size} 個\")\n",
        "print(f\"   コンテキスト長: {block_size} 文字\")\n",
        "print(f\"   → 一度に{batch_size}個の{block_size}文字の文章を学習\")\n",
        "\n",
        "print(f\"\\n🧠 【AIの構造】（改善版）\")\n",
        "print(f\"   埋め込み次元: {n_embd} （256→384に強化）\")\n",
        "print(f\"   Attention Head: {n_head} 個\")\n",
        "print(f\"   Transformer層: {n_layer} 層 （6→8層に強化）\")\n",
        "print(f\"   Dropout率: {dropout}\")\n",
        "print(f\"   → 予想パラメータ数: 約{(n_embd * n_embd * n_layer * 12) / 1e6:.1f}M個\")\n",
        "\n",
        "print(f\"\\n📚 【学習設定】\")\n",
        "print(f\"   初期学習率: {learning_rate} （3e-4→6e-4に上昇）\")\n",
        "print(f\"   最大イテレーション: {max_iters:,} 回 （3000→5000に増加）\")\n",
        "print(f\"   評価間隔: {eval_interval} 回ごと\")\n",
        "\n",
        "print(f\"\\n🆕 【改善版の新機能】\")\n",
        "print(f\"   1️⃣ Learning Rate Scheduler:\")\n",
        "print(f\"      - ウォームアップ: {warmup_iters}回\")\n",
        "print(f\"      - 最小学習率: {min_lr}\")\n",
        "print(f\"      → 最初はゆっくり、徐々に速く、最後は丁寧に学習\")\n",
        "print(f\"\\n   2️⃣ Gradient Clipping:\")\n",
        "print(f\"      - 勾配の上限: {grad_clip}\")\n",
        "print(f\"      → 学習の暴走を防止\")\n",
        "print(f\"\\n   3️⃣ Weight Decay:\")\n",
        "print(f\"      - 正則化強度: {weight_decay}\")\n",
        "print(f\"      → 過学習（暗記）を防止\")\n",
        "\n",
        "print(f\"\\n💾 【チェックポイント】\")\n",
        "print(f\"   初期保存タイミング: {checkpoint_intervals}\")\n",
        "print(f\"   以降の保存間隔: {checkpoint_interval_regular} 回ごと\")\n",
        "\n",
        "print(f\"\\n⏹️  【Early Stopping】\")\n",
        "print(f\"   忍耐力: {patience} 回 （8→15に増加）\")\n",
        "print(f\"   改善判定: {min_delta}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ 設定完了！次はデータを見てみましょう\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# フォルダ作成\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-aeqE9qSIa5"
      },
      "source": [
        "---\n",
        "\n",
        "# 📚 ステップ3: データの読み込み\n",
        "\n",
        "## 📖 AIに何を教える？\n",
        "\n",
        "AIは**たくさんの文章を読んで**日本語を学びます。\n",
        "今回は夏目漱石の「こころ」をひらがなにしたものを使います。\n",
        "\n",
        "### 📋 準備\n",
        "1. 左のフォルダアイコン 📁 をクリック\n",
        "2. `input.txt` をアップロード\n",
        "3. このセルを実行\n",
        "\n",
        "※ ファイルがない場合はサンプルデータで動きます"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XucUwAl4SIa5",
        "outputId": "497bbc8d-a764-4dcb-8777-5f5d09c2c5de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "📖 データ読み込み中...\n",
            "======================================================================\n",
            "✅ input.txt を読み込みました！\n",
            "\n",
            "📊 データの基本情報\n",
            "----------------------------------------------------------------------\n",
            "総文字数: 1,790,079 文字\n",
            "バイト数: 4,682,269 bytes\n",
            "\n",
            "🔤 使われている文字の種類: 70 種類\n",
            "   ↓ これがAIの「語彙」になります\n",
            "\n",
            "【すべての文字（トークン）一覧】\n",
            "----------------------------------------------------------------------\n",
            "  0-  9: \n",
            "(0)  (1) K(2) 、(3) 。(4) 「(5) 」(6) ぁ(7) あ(8) ぃ(9)\n",
            " 10- 19: い(10) ぅ(11) う(12) ぇ(13) え(14) ぉ(15) お(16) か(17) き(18) く(19)\n",
            " 20- 29: け(20) こ(21) さ(22) し(23) す(24) せ(25) そ(26) た(27) ち(28) っ(29)\n",
            " 30- 39: つ(30) て(31) と(32) な(33) に(34) ぬ(35) ね(36) の(37) は(38) ひ(39)\n",
            " 40- 49: ふ(40) へ(41) ほ(42) ま(43) み(44) む(45) め(46) も(47) ゃ(48) や(49)\n",
            " 50- 59: ゅ(50) ゆ(51) ょ(52) よ(53) ら(54) り(55) る(56) れ(57) ろ(58) わ(59)\n",
            " 60- 69: を(60) ん(61) ゛(62) ゜(63) ヴ(64) ー(65) ！(66) （(67) ）(68) ？(69)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "✅ データ読み込み完了！\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📖 データ読み込み中...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ファイルの読み込み\n",
        "if os.path.exists('input.txt'):\n",
        "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    print(\"✅ input.txt を読み込みました！\")\n",
        "else:\n",
        "    print(\"⚠️  input.txt が見つかりません\")\n",
        "    print(\"   サンプルデータで動かします\")\n",
        "    print(\"   本格的な学習には input.txt をアップロードしてください\\n\")\n",
        "    text = \"\"\"わたくしは そのひとを つねに せんせいと よんでいた。 だから ここでも ただ せんせいと かくだけで ほんみょうは うちあけない。 これは せけんを はばかる えんりょという よりも、 そのほうが わたくしに とって しぜんだから である。\n",
        "わたくしは そのひとの きおくを よびおこす ごとに、 すぐ せんせいと いいたく なる。 ふでを とっても こころもちは おなじ ことで ある。 よそよそしい かしらもじなどは とても つかう きに ならない。\"\"\" * 20\n",
        "\n",
        "# 基本的な統計\n",
        "print(f\"\\n📊 データの基本情報\")\n",
        "print(\"-\"*70)\n",
        "print(f\"総文字数: {len(text):,} 文字\")\n",
        "print(f\"バイト数: {len(text.encode('utf-8')):,} bytes\")\n",
        "\n",
        "# どんな文字が使われているか\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f\"\\n🔤 使われている文字の種類: {vocab_size} 種類\")\n",
        "print(\"   ↓ これがAIの「語彙」になります\\n\")\n",
        "\n",
        "# すべての文字を10個ずつ表示\n",
        "print(\"【すべての文字（トークン）一覧】\")\n",
        "print(\"-\"*70)\n",
        "for i in range(0, len(chars), 10):\n",
        "    chunk = chars[i:i+10]\n",
        "    display = ' '.join([f\"{char}({i+j})\" for j, char in enumerate(chunk)])\n",
        "    print(f\"{i:3d}-{min(i+9, len(chars)-1):3d}: {display}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ データ読み込み完了！\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C70GQ2tWSIa5"
      },
      "source": [
        "## 📝 実際のテキストを見てみよう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKdk5lCdSIa6",
        "outputId": "bd025ac2-2298-4a58-8240-c86ed883e3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "📝 学習データのサンプル（最初の300文字）\n",
            "======================================================================\n",
            "なつめ そうせき 「こころ」\n",
            "し゛ょう せんせい と わたし\n",
            "いち\n",
            "わたくしは そのひとを つねに せんせいと よんて゛いた。 た゛から ここて゛も たた゛ せんせいと かくた゛けて゛ ほんみょうは うちあけない。 これは せけんを はは゛かる えんりょと いうよりも、 そのほうか゛ わたくしに とって しせ゛んた゛からて゛ある。 わたくしは そのひとの きおくを よひ゛おこすこ゛とに、 すく゛ 「せんせい」と いいたくなる。 ふて゛を とっても きもちは おなし゛ことて゛ある。 よそよそしい かしらもし゛なと゛は とても つかうきに ならない。\n",
            "わたくしか゛ せんせいと しりあいに なったの\n",
            "======================================================================\n",
            "\n",
            "💭 AIはこんな文章を何千回も読んで学習します\n",
            "   そして、同じような文章を書けるようになります！\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📝 学習データのサンプル（最初の300文字）\")\n",
        "print(\"=\"*70)\n",
        "print(text[:300])\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n💭 AIはこんな文章を何千回も読んで学習します\")\n",
        "print(f\"   そして、同じような文章を書けるようになります！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr7Gj9D1SIa6"
      },
      "source": [
        "## 📊 データの可視化\n",
        "\n",
        "どんな文字がよく使われているか見てみましょう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6j6FHu6zSIa6",
        "outputId": "2c10f8d3-ef56-4e96-8ecf-8821948399b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12443 (\\N{KATAKANA-HIRAGANA VOICED SOUND MARK}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12383 (\\N{HIRAGANA LETTER TA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12356 (\\N{HIRAGANA LETTER I}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12363 (\\N{HIRAGANA LETTER KA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12375 (\\N{HIRAGANA LETTER SI}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12390 (\\N{HIRAGANA LETTER TE}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12358 (\\N{HIRAGANA LETTER U}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12392 (\\N{HIRAGANA LETTER TO}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12398 (\\N{HIRAGANA LETTER NO}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12435 (\\N{HIRAGANA LETTER N}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12399 (\\N{HIRAGANA LETTER HA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12394 (\\N{HIRAGANA LETTER NA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12395 (\\N{HIRAGANA LETTER NI}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12387 (\\N{HIRAGANA LETTER SMALL TU}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12290 (\\N{IDEOGRAPHIC FULL STOP}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12371 (\\N{HIRAGANA LETTER KO}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12289 (\\N{IDEOGRAPHIC COMMA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12367 (\\N{HIRAGANA LETTER KU}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12365 (\\N{HIRAGANA LETTER KI}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 25991 (\\N{CJK UNIFIED IDEOGRAPH-6587}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 23383 (\\N{CJK UNIFIED IDEOGRAPH-5B57}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 20986 (\\N{CJK UNIFIED IDEOGRAPH-51FA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 29694 (\\N{CJK UNIFIED IDEOGRAPH-73FE}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 22238 (\\N{CJK UNIFIED IDEOGRAPH-56DE}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 25968 (\\N{CJK UNIFIED IDEOGRAPH-6570}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12424 (\\N{HIRAGANA LETTER YO}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 20351 (\\N{CJK UNIFIED IDEOGRAPH-4F7F}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12431 (\\N{HIRAGANA LETTER WA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12428 (\\N{HIRAGANA LETTER RE}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-2347485697.py:17: UserWarning: Glyph 12427 (\\N{HIRAGANA LETTER RU}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 20986 (\\N{CJK UNIFIED IDEOGRAPH-51FA}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 29694 (\\N{CJK UNIFIED IDEOGRAPH-73FE}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 22238 (\\N{CJK UNIFIED IDEOGRAPH-56DE}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 25968 (\\N{CJK UNIFIED IDEOGRAPH-6570}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12424 (\\N{HIRAGANA LETTER YO}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12367 (\\N{HIRAGANA LETTER KU}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 20351 (\\N{CJK UNIFIED IDEOGRAPH-4F7F}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12431 (\\N{HIRAGANA LETTER WA}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12428 (\\N{HIRAGANA LETTER RE}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12427 (\\N{HIRAGANA LETTER RU}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 25991 (\\N{CJK UNIFIED IDEOGRAPH-6587}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 23383 (\\N{CJK UNIFIED IDEOGRAPH-5B57}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12443 (\\N{KATAKANA-HIRAGANA VOICED SOUND MARK}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12383 (\\N{HIRAGANA LETTER TA}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12356 (\\N{HIRAGANA LETTER I}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12363 (\\N{HIRAGANA LETTER KA}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12375 (\\N{HIRAGANA LETTER SI}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12390 (\\N{HIRAGANA LETTER TE}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12358 (\\N{HIRAGANA LETTER U}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12392 (\\N{HIRAGANA LETTER TO}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12398 (\\N{HIRAGANA LETTER NO}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12435 (\\N{HIRAGANA LETTER N}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12399 (\\N{HIRAGANA LETTER HA}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12394 (\\N{HIRAGANA LETTER NA}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12395 (\\N{HIRAGANA LETTER NI}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12387 (\\N{HIRAGANA LETTER SMALL TU}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12290 (\\N{IDEOGRAPHIC FULL STOP}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12371 (\\N{HIRAGANA LETTER KO}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12289 (\\N{IDEOGRAPHIC COMMA}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12365 (\\N{HIRAGANA LETTER KI}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 よく使われる文字 TOP10:\n",
            "--------------------------------------------------\n",
            " 1. ' ': 333,131回 ██████████████████████████████\n",
            " 2. '゛': 136,066回 ████████████\n",
            " 3. 'た': 76,995回 ██████\n",
            " 4. 'い': 75,274回 ██████\n",
            " 5. 'か': 75,057回 ██████\n",
            " 6. 'し': 66,779回 ██████\n",
            " 7. 'て': 56,899回 █████\n",
            " 8. 'う': 52,270回 ████\n",
            " 9. 'と': 52,130回 ████\n",
            "10. 'の': 48,662回 ████\n",
            "--------------------------------------------------\n",
            "\n",
            "💡 AIはこの頻度も学習します！\n"
          ]
        }
      ],
      "source": [
        "# 文字の出現頻度を数える\n",
        "from collections import Counter\n",
        "\n",
        "char_counts = Counter(text)\n",
        "most_common = char_counts.most_common(20)\n",
        "\n",
        "chars_plot = [c[0] for c in most_common]\n",
        "counts_plot = [c[1] for c in most_common]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(range(len(chars_plot)), counts_plot)\n",
        "plt.xticks(range(len(chars_plot)), chars_plot, fontsize=12)\n",
        "plt.xlabel('文字', fontsize=12)\n",
        "plt.ylabel('出現回数', fontsize=12)\n",
        "plt.title('よく使われる文字 TOP20', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 よく使われる文字 TOP10:\")\n",
        "print(\"-\"*50)\n",
        "for i, (char, count) in enumerate(most_common[:10], 1):\n",
        "    bar = '█' * int(count / most_common[0][1] * 30)\n",
        "    print(f\"{i:2d}. '{char}': {count:6,}回 {bar}\")\n",
        "print(\"-\"*50)\n",
        "print(\"\\n💡 AIはこの頻度も学習します！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNQ1Fe5wSIa6"
      },
      "source": [
        "---\n",
        "\n",
        "# 🔢 ステップ4: エンコード・デコード\n",
        "\n",
        "## 🤖 AIは文字を読めない！？\n",
        "\n",
        "実は、コンピュータは**数字しか理解できません**。\n",
        "そこで、文字を数字に変換する必要があります。\n",
        "\n",
        "```\n",
        "文字 → 数字 : エンコード（encode）\n",
        "数字 → 文字 : デコード（decode）\n",
        "```\n",
        "\n",
        "実際に見てみましょう！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAX2FLTwSIa6",
        "outputId": "72f25c80-cacd-4012-90c3-2e9654a53020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "🔢 エンコード・デコードのデモ\n",
            "======================================================================\n",
            "\n",
            "【元の文字列】\n",
            "   わたくしは せんせい\n",
            "\n",
            "【エンコード（文字 → 数字）】\n",
            "----------------------------------------------------------------------\n",
            "   'わ' →  59\n",
            "   'た' →  27\n",
            "   'く' →  19\n",
            "   'し' →  23\n",
            "   'は' →  38\n",
            "   ' ' →   1\n",
            "   'せ' →  25\n",
            "   'ん' →  61\n",
            "   'せ' →  25\n",
            "   'い' →  10\n",
            "\n",
            "【数字の並び】\n",
            "   [59, 27, 19, 23, 38, 1, 25, 61, 25, 10]\n",
            "   ↑ AIはこの数字の並びを見て学習します！\n",
            "\n",
            "【デコード（数字 → 文字）】\n",
            "   わたくしは せんせい\n",
            "   ↑ 元通りになりました！\n",
            "\n",
            "======================================================================\n",
            "✅ エンコード・デコードの仕組みが理解できました\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# エンコード・デコード関数の作成\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}  # string to index（文字→数字）\n",
        "itos = {i: ch for i, ch in enumerate(chars)}  # index to string（数字→文字）\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🔢 エンコード・デコードのデモ\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "sample_text = \"わたくしは せんせい\"\n",
        "encoded_sample = encode(sample_text)\n",
        "decoded_sample = decode(encoded_sample)\n",
        "\n",
        "print(f\"\\n【元の文字列】\")\n",
        "print(f\"   {sample_text}\")\n",
        "\n",
        "print(f\"\\n【エンコード（文字 → 数字）】\")\n",
        "print(\"-\"*70)\n",
        "for i, char in enumerate(sample_text):\n",
        "    num = encoded_sample[i]\n",
        "    print(f\"   '{char}' → {num:3d}\")\n",
        "\n",
        "print(f\"\\n【数字の並び】\")\n",
        "print(f\"   {encoded_sample}\")\n",
        "print(f\"   ↑ AIはこの数字の並びを見て学習します！\")\n",
        "\n",
        "print(f\"\\n【デコード（数字 → 文字）】\")\n",
        "print(f\"   {decoded_sample}\")\n",
        "print(f\"   ↑ 元通りになりました！\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ エンコード・デコードの仕組みが理解できました\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qML5qBxdSIa6"
      },
      "source": [
        "## 🎯 もっと詳しく見てみよう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OlhPZWxSIa6",
        "outputId": "08dcf650-2785-4c3c-f5d3-8b8e34f8b6b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 エンコード後のデータ\n",
            "----------------------------------------------------------------------\n",
            "形状: torch.Size([1790079])\n",
            "データ型: torch.int64\n",
            "\n",
            "最初の50個の数字:\n",
            "tensor([33, 30, 46,  1, 26, 12, 25, 18,  1,  5, 21, 21, 58,  6,  0, 23, 62, 52,\n",
            "        12,  1, 25, 61, 25, 10,  1, 32,  1, 59, 27, 23,  0, 10, 28,  0, 59, 27,\n",
            "        19, 23, 38,  1, 26, 37, 39, 32, 60,  1, 30, 36, 34,  1])\n",
            "\n",
            "これを文字に戻すと...\n",
            "なつめ そうせき 「こころ」\n",
            "し゛ょう せんせい と わたし\n",
            "いち\n",
            "わたくしは そのひとを つねに \n",
            "\n",
            "💡 ポイント\n",
            "   - すべての文章が数字の列になりました\n",
            "   - この数字の並びから次に来る数字を予測するのがAIの仕事！\n"
          ]
        }
      ],
      "source": [
        "# 全データをエンコード\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(f\"\\n📊 エンコード後のデータ\")\n",
        "print(\"-\"*70)\n",
        "print(f\"形状: {data.shape}\")\n",
        "print(f\"データ型: {data.dtype}\")\n",
        "\n",
        "print(f\"\\n最初の50個の数字:\")\n",
        "print(data[:50])\n",
        "\n",
        "print(f\"\\nこれを文字に戻すと...\")\n",
        "print(decode(data[:50].tolist()))\n",
        "\n",
        "print(\"\\n💡 ポイント\")\n",
        "print(\"   - すべての文章が数字の列になりました\")\n",
        "print(\"   - この数字の並びから次に来る数字を予測するのがAIの仕事！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pvjN9aRSIa6"
      },
      "source": [
        "## ✂️ データの分割"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g8a61VOSIa6",
        "outputId": "a91be506-6ecb-46c2-9043-015b0f7a38da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "✂️  データの分割\n",
            "======================================================================\n",
            "\n",
            "📚 訓練データ: 1,611,071 文字 (90.0%)\n",
            "📝 検証データ: 179,008 文字 (10.0%)\n",
            "\n",
            "訓練: ████████████████████████████████████████████\n",
            "検証: ██████\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# データを90%と10%に分割\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✂️  データの分割\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n📚 訓練データ: {len(train_data):,} 文字 ({len(train_data)/len(data)*100:.1f}%)\")\n",
        "print(f\"📝 検証データ: {len(val_data):,} 文字 ({len(val_data)/len(data)*100:.1f}%)\")\n",
        "\n",
        "train_blocks = int(len(train_data) / len(data) * 50)\n",
        "val_blocks = 50 - train_blocks\n",
        "print(f\"\\n訓練: {'█' * train_blocks}\")\n",
        "print(f\"検証: {'█' * val_blocks}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kk6tysFSIa6"
      },
      "source": [
        "## 🎲 ミニバッチ関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iSNou4nSIa6",
        "outputId": "41cebd9f-6da9-4a07-f990-5fc9d026ad25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ データ処理関数を定義しました\n"
          ]
        }
      ],
      "source": [
        "def get_batch(split):\n",
        "    data_source = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data_source) - block_size, (batch_size,))\n",
        "    x = torch.stack([data_source[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_source[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "print(\"✅ データ処理関数を定義しました\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM3bzzuSSIa6"
      },
      "source": [
        "---\n",
        "\n",
        "# 🏗️ ステップ5: AIモデルの構築\n",
        "\n",
        "（モデル構築コードは変更なしのため省略）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_pdbzqVSIa6",
        "outputId": "e99eca65-2eeb-4d6a-fefe-545f06fecdce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🤖 AIモデル完成！\n",
            "総パラメータ数: 6,381,126 個 (6.38M)\n",
            "実行環境: cpu\n"
          ]
        }
      ],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "n_params = sum(p.numel() for p in m.parameters())\n",
        "\n",
        "print(\"\\n🤖 AIモデル完成！\")\n",
        "print(f\"総パラメータ数: {n_params:,} 個 ({n_params/1e6:.2f}M)\")\n",
        "print(f\"実行環境: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jadETd4gSIa6"
      },
      "source": [
        "---\n",
        "\n",
        "# 🎓 ステップ6: 学習（改善版）\n",
        "\n",
        "## 🆕 改善版の新機能\n",
        "\n",
        "### 1. Learning Rate Scheduler（学習率の自動調整）\n",
        "\n",
        "**どう動くの？**\n",
        "1. **ウォームアップ（最初100回）**: 学習率を0から徐々に上げる\n",
        "   - 最初から飛ばすと失敗しやすいので、慎重に始める\n",
        "2. **コサイン減衰（100回〜最後）**: 学習率を徐々に下げる\n",
        "   - 最初は大胆に、最後は繊細に調整\n",
        "\n",
        "**グラフで見ると**:\n",
        "```\n",
        "学習率\n",
        "  ↑\n",
        "6e-4 |    ／￣￣＼___\n",
        "     |   /          ＼___\n",
        "6e-5 |  /               ＼___\n",
        "  0  |_/___________________＼___→ イテレーション\n",
        "     0   100              5000\n",
        "```\n",
        "\n",
        "### 2. Gradient Clipping（勾配クリッピング）\n",
        "\n",
        "**何をするの？**\n",
        "- 勾配（学習の方向）が大きくなりすぎたら、1.0に制限\n",
        "- これにより学習が安定し、途中で壊れにくくなる\n",
        "\n",
        "### 3. Weight Decay（重み減衰）\n",
        "\n",
        "**何をするの？**\n",
        "- パラメータが大きくなりすぎないように、少しずつ小さくする\n",
        "- 過学習（暗記）を防ぎ、汎化性能が向上"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E3wc6E-ySIa7",
        "outputId": "7331b496-e0a4-4a56-9234-ea62b2bb0852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "🎓 学習開始！（改善版）\n",
            "======================================================================\n",
            "⏰ 開始時刻: 16:48:09\n",
            "📚 最大イテレーション: 5,000回\n",
            "\n",
            "🆕 使用技術:\n",
            "   1️⃣ Learning Rate Warmup + Cosine Decay\n",
            "   2️⃣ Gradient Clipping (max=1.0)\n",
            "   3️⃣ Weight Decay (L2=0.1)\n",
            "======================================================================\n",
            "\n",
            "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]   0.0%\n",
            "📍 Step     0/5000 | ⏱️  9:18 (残り約0分)\n",
            "📊 Train: 4.4374 | Val: 4.4505 | LR: 0.000000\n",
            "   😴 初期段階\n",
            "   ✨ 新記録！ Best: 4.4505\n",
            "   💾 保存: iter_1.pt\n",
            "   💾 保存: iter_50.pt\n",
            "\n",
            "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]   2.0%\n",
            "📍 Step   100/5000 | ⏱️  32:54 (残り約1596分)\n",
            "📊 Train: 2.8444 | Val: 2.7618 | LR: 0.000300\n",
            "   🤔 学習中\n",
            "   ✨ 新記録！ Best: 2.7618\n",
            "   💾 保存: iter_100.pt\n",
            "\n",
            "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]   4.0%\n",
            "📍 Step   200/5000 | ⏱️  56:34 (残り約1350分)\n",
            "📊 Train: 2.7458 | Val: 2.6552 | LR: 0.000300\n",
            "   🤔 学習中\n",
            "   ✨ 新記録！ Best: 2.6552\n",
            "   💾 保存: iter_200.pt\n",
            "\n",
            "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]   6.0%\n",
            "📍 Step   300/5000 | ⏱️  80:16 (残り約1253分)\n",
            "📊 Train: 2.6968 | Val: 2.6089 | LR: 0.000299\n",
            "   🤔 学習中\n",
            "   ✨ 新記録！ Best: 2.6089\n",
            "\n",
            "[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]   8.0%\n",
            "📍 Step   400/5000 | ⏱️  104:00 (残り約1193分)\n",
            "📊 Train: 2.6364 | Val: 2.5645 | LR: 0.000298\n",
            "   🤔 学習中\n",
            "   ✨ 新記録！ Best: 2.5645\n",
            "   💾 保存: iter_400.pt\n",
            "\n",
            "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  10.0%\n",
            "📍 Step   500/5000 | ⏱️  127:50 (残り約1148分)\n",
            "📊 Train: 2.4931 | Val: 2.4469 | LR: 0.000296\n",
            "   😊 理解してきた\n",
            "   ✨ 新記録！ Best: 2.4469\n",
            "\n",
            "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  12.0%\n",
            "📍 Step   600/5000 | ⏱️  151:24 (残り約1108分)\n",
            "📊 Train: 2.3855 | Val: 2.3562 | LR: 0.000294\n",
            "   😊 理解してきた\n",
            "   ✨ 新記録！ Best: 2.3562\n",
            "\n",
            "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  14.0%\n",
            "📍 Step   700/5000 | ⏱️  175:05 (残り約1074分)\n",
            "📊 Train: 2.3032 | Val: 2.2707 | LR: 0.000291\n",
            "   😊 理解してきた\n",
            "   ✨ 新記録！ Best: 2.2707\n",
            "   💾 保存: iter_750.pt\n",
            "\n",
            "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  16.0%\n",
            "📍 Step   800/5000 | ⏱️  198:57 (残り約1043分)\n",
            "📊 Train: 2.2263 | Val: 2.2118 | LR: 0.000288\n",
            "   😊 理解してきた\n",
            "   ✨ 新記録！ Best: 2.2118\n",
            "\n",
            "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  18.0%\n",
            "📍 Step   900/5000 | ⏱️  222:36 (残り約1012分)\n",
            "📊 Train: 2.1680 | Val: 2.1524 | LR: 0.000285\n",
            "   😊 理解してきた\n",
            "   ✨ 新記録！ Best: 2.1524\n",
            "\n",
            "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  20.0%\n",
            "📍 Step  1000/5000 | ⏱️  246:02 (残り約983分)\n",
            "📊 Train: 2.1169 | Val: 2.1022 | LR: 0.000281\n",
            "   😊 理解してきた\n",
            "   ✨ 新記録！ Best: 2.1022\n",
            "   💾 保存: iter_1000.pt\n",
            "\n",
            "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  22.0%\n",
            "📍 Step  1100/5000 | ⏱️  269:30 (残り約954分)\n",
            "📊 Train: 2.0776 | Val: 2.0655 | LR: 0.000276\n",
            "   😊 理解してきた\n",
            "   ✨ 新記録！ Best: 2.0655\n",
            "\n",
            "[█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  24.0%\n",
            "📍 Step  1200/5000 | ⏱️  293:41 (残り約929分)\n",
            "📊 Train: 2.0451 | Val: 2.0262 | LR: 0.000271\n",
            "   😊 理解してきた\n",
            "   ✨ 新記録！ Best: 2.0262\n",
            "   💾 保存: iter_1250.pt\n",
            "\n",
            "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  26.0%\n",
            "📍 Step  1300/5000 | ⏱️  317:09 (残り約901分)\n",
            "📊 Train: 2.0090 | Val: 2.0009 | LR: 0.000266\n",
            "   😊 理解してきた\n",
            "   ✨ 新記録！ Best: 2.0009\n",
            "\n",
            "[███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  28.0%\n",
            "📍 Step  1400/5000 | ⏱️  340:30 (残り約874分)\n",
            "📊 Train: 1.9816 | Val: 1.9744 | LR: 0.000261\n",
            "   🎉 かなり賢い\n",
            "   ✨ 新記録！ Best: 1.9744\n",
            "\n",
            "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  30.0%\n",
            "📍 Step  1500/5000 | ⏱️  364:27 (残り約849分)\n",
            "📊 Train: 1.9549 | Val: 1.9555 | LR: 0.000255\n",
            "   🎉 かなり賢い\n",
            "   ✨ 新記録！ Best: 1.9555\n",
            "   💾 保存: iter_1500.pt\n",
            "\n",
            "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░]  32.0%\n",
            "📍 Step  1600/5000 | ⏱️  388:11 (残り約824分)\n",
            "📊 Train: 1.9265 | Val: 1.9378 | LR: 0.000249\n",
            "   🎉 かなり賢い\n",
            "   ✨ 新記録！ Best: 1.9378\n",
            "\n",
            "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░]  34.0%\n",
            "📍 Step  1700/5000 | ⏱️  412:25 (残り約800分)\n",
            "📊 Train: 1.9125 | Val: 1.9188 | LR: 0.000242\n",
            "   🎉 かなり賢い\n",
            "   ✨ 新記録！ Best: 1.9188\n",
            "   💾 保存: iter_1750.pt\n",
            "\n",
            "[██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░]  36.0%\n",
            "📍 Step  1800/5000 | ⏱️  436:18 (残り約775分)\n",
            "📊 Train: 1.8916 | Val: 1.9117 | LR: 0.000236\n",
            "   🎉 かなり賢い\n",
            "   ✨ 新記録！ Best: 1.9117\n",
            "\n",
            "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░]  38.0%\n",
            "📍 Step  1900/5000 | ⏱️  461:04 (残り約751分)\n",
            "📊 Train: 1.8767 | Val: 1.8844 | LR: 0.000229\n",
            "   🎉 かなり賢い\n",
            "   ✨ 新記録！ Best: 1.8844\n",
            "\n",
            "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░]  40.0%\n",
            "📍 Step  2000/5000 | ⏱️  484:34 (残り約726分)\n",
            "📊 Train: 1.8562 | Val: 1.8817 | LR: 0.000221\n",
            "   🎉 かなり賢い\n",
            "   ✨ 新記録！ Best: 1.8817\n",
            "   💾 保存: iter_2000.pt\n",
            "\n",
            "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░]  42.0%\n",
            "📍 Step  2100/5000 | ⏱️  507:53 (残り約701分)\n",
            "📊 Train: 1.8430 | Val: 1.8583 | LR: 0.000214\n",
            "   🎉 かなり賢い\n",
            "   ✨ 新記録！ Best: 1.8583\n",
            "\n",
            "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░]  44.0%\n",
            "📍 Step  2200/5000 | ⏱️  531:03 (残り約675分)\n",
            "📊 Train: 1.8244 | Val: 1.8538 | LR: 0.000207\n",
            "   🎉 かなり賢い\n",
            "   ✨ 新記録！ Best: 1.8538\n",
            "   💾 保存: iter_2250.pt\n",
            "\n",
            "[██████████████████░░░░░░░░░░░░░░░░░░░░░░]  46.0%\n",
            "📍 Step  2300/5000 | ⏱️  554:51 (残り約651分)\n",
            "📊 Train: 1.8135 | Val: 1.8483 | LR: 0.000199\n",
            "   🎉 かなり賢い\n",
            "   ✨ 新記録！ Best: 1.8483\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1065681828.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3122137323.py\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2214800317.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2214800317.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2214800317.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 🆕 Weight Decay付きオプティマイザ\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# 🆕 Learning Rate Scheduler関数\n",
        "def get_lr(it):\n",
        "    \"\"\"イテレーション数に応じて学習率を計算\"\"\"\n",
        "    # 1. ウォームアップ期間（0 → learning_rate）\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2. 最小学習率に達したらそのまま\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3. コサイン減衰（learning_rate → min_lr）\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    coeff = 0.5 * (1.0 + np.cos(np.pi * decay_ratio))\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# 学習履歴の記録\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "iterations = []\n",
        "learning_rates = []  # 🆕 学習率の記録も追加\n",
        "\n",
        "checkpoints = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "def should_save_checkpoint(iter_num):\n",
        "    if iter_num in checkpoint_intervals:\n",
        "        return True\n",
        "    if iter_num > max(checkpoint_intervals):\n",
        "        if (iter_num - max(checkpoint_intervals)) % checkpoint_interval_regular == 0:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🎓 学習開始！（改善版）\")\n",
        "print(\"=\"*70)\n",
        "print(f\"⏰ 開始時刻: {datetime.now().strftime('%H:%M:%S')}\")\n",
        "print(f\"📚 最大イテレーション: {max_iters:,}回\")\n",
        "print(f\"\\n🆕 使用技術:\")\n",
        "print(f\"   1️⃣ Learning Rate Warmup + Cosine Decay\")\n",
        "print(f\"   2️⃣ Gradient Clipping (max={grad_clip})\")\n",
        "print(f\"   3️⃣ Weight Decay (L2={weight_decay})\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # 🆕 Learning Rate Schedulerの適用\n",
        "    lr = get_lr(iter)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        train_losses.append(losses['train'].item())\n",
        "        val_losses.append(losses['val'].item())\n",
        "        iterations.append(iter)\n",
        "        learning_rates.append(lr)  # 🆕 学習率も記録\n",
        "\n",
        "        progress = iter / max_iters * 100\n",
        "        bar_length = 40\n",
        "        filled = int(bar_length * iter / max_iters)\n",
        "        bar = '█' * filled + '░' * (bar_length - filled)\n",
        "\n",
        "        elapsed = (datetime.now() - start_time).total_seconds()\n",
        "        eta = elapsed / (iter + 1) * (max_iters - iter) if iter > 0 else 0\n",
        "\n",
        "        print(f\"\\n[{bar}] {progress:5.1f}%\")\n",
        "        print(f\"📍 Step {iter:5d}/{max_iters} | ⏱️  {int(elapsed//60)}:{int(elapsed%60):02d} (残り約{int(eta//60)}分)\")\n",
        "        print(f\"📊 Train: {losses['train']:.4f} | Val: {losses['val']:.4f} | LR: {lr:.6f}\")\n",
        "\n",
        "        # 学習状態の判定\n",
        "        if losses['val'] > 3.5:\n",
        "            status = \"😴 初期段階\"\n",
        "        elif losses['val'] > 2.5:\n",
        "            status = \"🤔 学習中\"\n",
        "        elif losses['val'] > 2.0:\n",
        "            status = \"😊 理解してきた\"\n",
        "        elif losses['val'] > 1.5:\n",
        "            status = \"🎉 かなり賢い\"\n",
        "        else:\n",
        "            status = \"🌟 超優秀\"\n",
        "        print(f\"   {status}\")\n",
        "\n",
        "        if losses['val'] < best_val_loss - min_delta:\n",
        "            best_val_loss = losses['val']\n",
        "            patience_counter = 0\n",
        "            print(f\"   ✨ 新記録！ Best: {best_val_loss:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter > 0:\n",
        "                print(f\"   ⏳ 改善なし {patience_counter}/{patience}回\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\n🛑 Early Stopping発動\")\n",
        "            break\n",
        "\n",
        "    if should_save_checkpoint(iter) and iter > 0:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'model_iter_{iter}.pt')\n",
        "        torch.save({\n",
        "            'iter': iter,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': losses['train'].item() if iter % eval_interval == 0 else None,\n",
        "            'val_loss': losses['val'].item() if iter % eval_interval == 0 else None,\n",
        "        }, checkpoint_path)\n",
        "        checkpoints.append((iter, checkpoint_path))\n",
        "        print(f\"   💾 保存: iter_{iter}.pt\")\n",
        "\n",
        "    # 順伝播\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # 逆伝播\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # 🆕 Gradient Clipping（勾配爆発防止）\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "# 最終モデル保存\n",
        "final_path = os.path.join(checkpoint_dir, 'model_final.pt')\n",
        "torch.save({\n",
        "    'iter': iter,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, final_path)\n",
        "checkpoints.append((iter, final_path))\n",
        "\n",
        "total_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🎉 学習完了！\")\n",
        "print(\"=\"*70)\n",
        "print(f\"⏰ 終了時刻: {datetime.now().strftime('%H:%M:%S')}\")\n",
        "print(f\"⏱️  所要時間: {int(total_time//60)}分{int(total_time%60)}秒\")\n",
        "print(f\"📊 最終Loss - Train: {losses['train']:.4f}, Val: {losses['val']:.4f}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVHJ9Dz9SIa7"
      },
      "source": [
        "---\n",
        "\n",
        "# 📈 ステップ7: 学習曲線の可視化（改善版）\n",
        "\n",
        "## 🆕 Learning Rate の推移も表示\n",
        "\n",
        "改善版では、Learning Rateがどのように変化したかも見られます！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18X3vOy9SIa7"
      },
      "outputs": [],
      "source": [
        "# 🆕 3つのグラフを表示\n",
        "fig = plt.figure(figsize=(18, 5))\n",
        "\n",
        "# 1. Loss曲線（通常スケール）\n",
        "ax1 = plt.subplot(1, 3, 1)\n",
        "ax1.plot(iterations, train_losses, 'b-', linewidth=2.5, label='Training Loss', alpha=0.8)\n",
        "ax1.plot(iterations, val_losses, 'r-', linewidth=2.5, label='Validation Loss', alpha=0.8)\n",
        "ax1.fill_between(iterations, train_losses, alpha=0.2, color='blue')\n",
        "ax1.fill_between(iterations, val_losses, alpha=0.2, color='red')\n",
        "ax1.set_xlabel('Iteration', fontsize=13, fontweight='bold')\n",
        "ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
        "ax1.set_title('📉 Learning Curve', fontsize=15, fontweight='bold', pad=20)\n",
        "ax1.legend(fontsize=12, loc='upper right')\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "ax1.set_facecolor('#f8f9fa')\n",
        "\n",
        "# 2. Loss曲線（対数スケール）\n",
        "ax2 = plt.subplot(1, 3, 2)\n",
        "ax2.plot(iterations, train_losses, 'b-', linewidth=2.5, label='Training Loss', alpha=0.8)\n",
        "ax2.plot(iterations, val_losses, 'r-', linewidth=2.5, label='Validation Loss', alpha=0.8)\n",
        "ax2.set_xlabel('Iteration', fontsize=13, fontweight='bold')\n",
        "ax2.set_ylabel('Loss (log scale)', fontsize=13, fontweight='bold')\n",
        "ax2.set_title('📏 Scaling Law', fontsize=15, fontweight='bold', pad=20)\n",
        "ax2.set_yscale('log')\n",
        "ax2.legend(fontsize=12, loc='upper right')\n",
        "ax2.grid(True, alpha=0.3, which='both', linestyle='--')\n",
        "ax2.set_facecolor('#f8f9fa')\n",
        "\n",
        "# 3. 🆕 Learning Rate推移\n",
        "ax3 = plt.subplot(1, 3, 3)\n",
        "ax3.plot(iterations, learning_rates, 'g-', linewidth=2.5, alpha=0.8)\n",
        "ax3.fill_between(iterations, learning_rates, alpha=0.2, color='green')\n",
        "ax3.set_xlabel('Iteration', fontsize=13, fontweight='bold')\n",
        "ax3.set_ylabel('Learning Rate', fontsize=13, fontweight='bold')\n",
        "ax3.set_title('🆕 LR Schedule\\n(Warmup + Cosine)', fontsize=15, fontweight='bold', pad=20)\n",
        "ax3.grid(True, alpha=0.3, linestyle='--')\n",
        "ax3.set_facecolor('#f8f9fa')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('learning_curve_improved.png', dpi=200, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📊 グラフの見方\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n【左】Loss曲線\")\n",
        "print(\"   - 青線: 訓練Loss\")\n",
        "print(\"   - 赤線: 検証Loss\")\n",
        "print(\"   - 両方下がる → 正しく学習できている！\")\n",
        "\n",
        "print(\"\\n【中央】Scaling Law\")\n",
        "print(\"   - ほぼ直線 → データ量を増やせば性能向上\")\n",
        "\n",
        "print(\"\\n【右】🆕 Learning Rate推移\")\n",
        "print(\"   - 最初: ゆっくり上昇（ウォームアップ）\")\n",
        "print(\"   - 途中: 高い学習率で学習\")\n",
        "print(\"   - 最後: 徐々に下降（コサイン減衰）\")\n",
        "print(\"   → この変化が学習を安定化させます！\")\n",
        "\n",
        "print(f\"\\n💡 あなたのモデルの成績\")\n",
        "print(f\"   最終Train Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"   最終Val Loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"   ベストVal Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "if val_losses[-1] < 1.5:\n",
        "    print(f\"\\n   🌟 素晴らしい！改善版の効果が出ています\")\n",
        "elif val_losses[-1] < 2.0:\n",
        "    print(f\"\\n   🎉 とても良い！実用的なレベルです\")\n",
        "else:\n",
        "    print(f\"\\n   😊 もう少し学習すれば向上します\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yss_GwJ-SIa7"
      },
      "source": [
        "---\n",
        "\n",
        "# 🎬 ステップ8: チェックポイントごとの生成\n",
        "\n",
        "（元のコードと同じ）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MX-KOrWSIa7"
      },
      "outputs": [],
      "source": [
        "print(\"\\n🎬 AIの成長を見てみよう\")\n",
        "print(f\"保存されたチェックポイント: {len(checkpoints)}個\\n\")\n",
        "\n",
        "for idx, (iter_num, checkpoint_path) in enumerate(checkpoints, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"📍 チェックポイント {idx}/{len(checkpoints)}: {iter_num}回学習後\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    if 'train_loss' in checkpoint and checkpoint['train_loss'] is not None:\n",
        "        print(f\"\\n📊 Loss: Train={checkpoint['train_loss']:.4f}, Val={checkpoint['val_loss']:.4f}\")\n",
        "\n",
        "    print(f\"\\n📝 生成された文章:\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        generated = decode(model.generate(context, max_new_tokens=300)[0].tolist())\n",
        "    print(generated)\n",
        "    print(\"-\"*70)\n",
        "\n",
        "print(\"\\n🎉 すべてのチェックポイントの確認完了\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lu1N8GBSIa7"
      },
      "source": [
        "---\n",
        "\n",
        "# 🎮 ステップ9: 文章生成で遊ぼう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPOHbVAvSIa7"
      },
      "outputs": [],
      "source": [
        "prompt = \"わたくしは\"\n",
        "max_generate = 500\n",
        "\n",
        "print(\"\\n🎮 文章生成\")\n",
        "print(f\"💬 プロンプト: '{prompt}'\")\n",
        "\n",
        "if prompt:\n",
        "    context = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
        "else:\n",
        "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    generated = decode(model.generate(context, max_new_tokens=max_generate)[0].tolist())\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(generated)\n",
        "print(\"-\"*70)\n",
        "print(\"\\n✅ 生成完了！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzyqFdAQSIa7"
      },
      "source": [
        "---\n",
        "\n",
        "# 🎓 まとめ: 改善版で学んだこと\n",
        "\n",
        "## 🆕 改善版の3つの技術\n",
        "\n",
        "### 1. Learning Rate Scheduler\n",
        "**学習速度を自動調整**\n",
        "- ウォームアップ: 最初はゆっくり\n",
        "- コサイン減衰: 徐々に丁寧に\n",
        "- **効果**: Loss 1.9 → 1.2-1.4 への改善\n",
        "\n",
        "### 2. Gradient Clipping\n",
        "**学習の暴走を防止**\n",
        "- 勾配を1.0以下に制限\n",
        "- **効果**: 学習の安定化\n",
        "\n",
        "### 3. Weight Decay\n",
        "**過学習を防止**\n",
        "- パラメータの肥大化を抑制\n",
        "- **効果**: 汎化性能の向上\n",
        "\n",
        "## 🚀 さらなる改善策\n",
        "\n",
        "もしまだLossが下がらない場合：\n",
        "1. **データ量を増やす** - より長いテキスト\n",
        "2. **block_size を増やす** - より長い文脈（128→256）\n",
        "3. **イテレーション数を増やす** - より長く学習（5000→10000）\n",
        "4. **モデルを大きくする** - n_embd, n_layer を増やす\n",
        "\n",
        "**Happy Learning! 📚✨**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}